---
title: "Mining Software Engineering Data from GitHub"
author: "Georgios Gousios and Diomidis Spinellis"
date: "May 23, 2017"
output:
   revealjs::revealjs_presentation:
    theme: white
    css: theme.css
    center: false
    reveal_options:
      slideNumber: true
      previewLinks: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(sql.max.print = 5)

# Setup DB connection for use in the code blocks below
library(DBI)
db <- dbConnect(RSQLite::SQLite(), dbname = "rxjs-ghtorrent.db")
```

## GitHub and their data

With 40M repos and 15M users, [GitHub](http://github.com) is the largest source
code archive and one of the largest online collaboration platforms on the
planet.

For software engineering researchers, GitHub is cool because:

* [api.github.com](https://api.github.com)
* process and product data
* interconnection

## GitHub's API

Contains all data from all public repositories

* *Events*: A real-time endpoint of all things happening on GitHub

* *Entities*: Represent the state of a resource at the time of query

## Ways of accessing GitHub data

* [REST API / GraphQL](https://developer.github.com)
* [GitHub Archive](http://githubarchive.org) Collects GitHub events and offers
them over BigQuery
* [Github on BigQueury](https://cloud.google.com/bigquery/public-data/github) 
The source code of all public GitHub repos along with metadata on BigQuery.
* [GHTorrent](http://ghtorrent.org) Collects GitHub events, resolves all
entities linked from those and creates a relational view. Data is offered as
downloads, online access services or over BigQuery.

## The REST API

The REST API allows us to browse entities given a known starting point

```{bash, echo=T}
# Find Microsoft's 5 most starred repos
curl -s "https://api.github.com/orgs/microsoft/repos?per_page=100&page=1" | 
jq 'sort_by(.stargazers_count) |.[] | [.name, .stargazers_count] | @csv'| 
sed -e 's/^"\(.*\)"$/\1/' |tr -d '\' | 
tail -n 5
```

* An API key is needed, 5k reqs/hour
* Always use in combination with `per_page=100`
* Too fine-grained for many MSR tasks

## GraphQL API

GraphQL API allows us to query entities and their dependencies in one go

```javascript
{
  user(login: "gousiosg") {
    name
    location
    organizations(first:10) {
      nodes {
        name
        repositories(first: 20) {
          nodes {
            name
          }
        }
      }
    }
  }
}
```

## GitHub Archive

![GitHub Archive](gharchive.png)

Collects all GitHub **events** since late 2011, allows querying over Google
BigQuery.

## GitHub data on BigQuery

An updating snapshot of both code + metadata for 2.5M repos. Allows for new
types of research:

* What are the most popular testing frameworks for Python?

```sql
SELECT lib, count(*) AS count
FROM (
  SELECT REGEXP_EXTRACT(versions, r"(\w*)") as lib
  from (
    SELECT first(split(c.content, '\n')) as versions
    FROM (
        SELECT * 
        FROM [bigquery-public-data:github_repos.files]
        WHERE path contains 'requirements.txt'
      ) AS f
      JOIN 
         [bigquery-public-data:github_repos.contents] c on f.id = c.id
    ) AS v
  WHERE REGEXP_MATCH(versions, "[>=]=[0-9]*")
) AS l
GROUP BY lib
ORDER BY count DESC
```

## GitHub data on BigQuery

Which repos are affected by our vulnerability disclosure (Operation Rosehub)?

```sql
SELECT pop, repo_name, path
FROM (
  SELECT id, repo_name, path
  FROM `bigquery-public-data.github_repos.files` AS files
  WHERE path LIKE '%pom.xml' AND
    EXISTS (
      SELECT 1
      FROM `bigquery-public-data.github_repos.contents`
      WHERE NOT binary AND
        content LIKE '%commons-collections<%' AND
        content LIKE '%>3.2.1<%' AND
        id = files.id
    )
)
JOIN (
  SELECT
    difference.new_sha1 AS id,
    ARRAY_LENGTH(repo_name) AS pop
  FROM `bigquery-public-data.github_repos.commits`
  CROSS JOIN UNNEST(difference) AS difference
)
USING (id)
ORDER BY pop DESC;
```

Checkout the work of [Felipe Hoffa](https://medium.com/google-cloud/github-on-bigquery-analyze-all-the-code-b3576fd2b150)

## GHTorrent

GHTorrent collects all data, both events and entities, from the GitHub REST API 
and makes them available:

* As queriable [MySQL](http://ghtorrent.org/mysql.html) and 
[MongoDB](http://ghtorrent.org/raw.html) databases
* As [database dumps](http://ghtorrent.org/downloads.html) for both databases
* Over Google [BigQuery](https://bigquery.cloud.google.com/dataset/ghtorrent-bq:ght_2017_01_19) (MySQL data only)
* As continuously updating data streams (also, over [Google Pub/Sub](https://console.cloud.google.com/cloudpubsub/topicList?project=ghtorrent-bq) )

## Some statistics about GHTorrent

* $>$ 15TB of MongoDB (raw) data
* $>$ 5B rows in MySQL
* 140k API reqs/hour
* 70+ users donated API keys
* 300 users, from > 200 institutions have access
* 100+ papers

## Growth

This is how fast MongoDB grows (x1M)

| Entity| 2013 | Feb 2016 | Apr 2017 | $\Delta$ 2016 - 2017 |
|:----- |:----:|:----:|:----:|:--------------------:|
|Events | 43 | 476 | 886 | 1.9x  |
|Users | 0.7 | 6.7 | 13.5 | 2x|
|Repos | 1.3| 28 | 57 | 2x |
|Commits | 29.9 | 367 | 662 | 1.8x |
|Issues | 2.3 | 24,1 | 41.1 | 1.7x |
|Pull requests | 1.1 | 11.9 | 23.8 | 2x |
|Issue Comments | 2.8 |42 | 74.2 | 1.7x |
|Watchers | 7.7 | 51 | 84.9 | 1.6x |

GitHub/GHTorrent doubled its size during the last 14 months!

## Data collection

![Retrieval scheme](retrieval.png){ width=80% }

GHTorrent follows the event stream and recursively retrieves linked
entities.

## Distributed operation

![Retrieval scheme](distributed.png){ width=90% }

* _Event retrieval_ nodes query the event API
* _Data retrieval_ nodes apply the recursive descent retrieval process
    * One API key per data retrieval node

## Collection modes

* **Normal operation**: Follow the event timeline, apply dependency-based retrieval
* **Periodic updates**: Refresh the state of all repos/users (cater for deleted repos, changed user locations etc)
* **Full retrievals**: Get all info for a repo/user, in case some information is missing

## Important tables -- Users
![Users](users.png){ width=50% }

* Type: USR or ORG
* Fake users represent emails with corresponding GitHub account
* ~15% users are geolocated
* Emails and real names not distributed by default

## Important tables -- Projects & Commits
![projects commits](projects-commits.png){ width=60% }

* A fork has a non-NULL `forked_from` and `forked_commit_id`
* `language` in projects represents the primary language at collection time. More details in table `project_languages`
* A commit is only stored once per SHA
* The table `project_commits` links commits to projects

## Important tables -- Pull Requests

* If `head_repo_id` is null $\rightarrow$ the fork was deleted
* Entries in `pull_request_history` represent a log of actions on a PR
* PRs and issues are dual on GitHub
    * The PR history is also reflected in `issue_events`
* A PR is associated with 3 times of comments:
    * `issue_comments`: The PR/Issue discussion
    * `pull_request_comments`: PR-global code review comments
    * `commit_comments`: Line-local code review comments
* A PR has associated `pull_request_commits` (not retrievable in case of rebases)

##  SQL Schema: Things to remember

* The info in `organization_members` and `project_members` is not trustworthy,
as it is optional for orgs/projects to publish it
* `watchers` means GitHub stars
* `watchers` and `followers` are append-only tables (GitHub does not emit events on delete/update)
* `followers` is only updated during periodic updates, as GitHub stopped emitting
`FollowEvent`s
* The timestamps in `followers` and `watchers` were relative to the time of
retrieval. Since mid-2015 they are accurate.

## Get all commits for a non-fork
```{sql, connection=db, echo=T}
select u1.login as author, c.sha, c.created_at
from projects p, users u, users u1, commits c, project_commits pc
where pc.commit_id = c.id
  and pc.project_id = p.id
  and u1.id = c.author_id
  and u.id = p.owner_id
  and u.login = 'ReactiveX'
  and p.name = 'rxjs'
order by c.created_at desc
```

## Get all commits for a fork (1)
```{sql, connection=db, echo=T}
select c.id, u1.login as author, c.sha, c.created_at
from projects p, users u, users u1, commits c, project_commits pc
where pc.commit_id = c.id
  and pc.project_id = p.id
  and u1.id = c.author_id
  and u.id = p.owner_id
  and u.login = 'herflis'
  and p.name = 'rxjs-1'
order by c.created_at desc
```

**Q**: Why do we only get 2 commits?


## Get all commits for a fork (2)

What the previous query returns is a list of commits up to the fork point.
We also need to retrieve the base repository commits up to the fork point.

First, we get some information to make querying simpler

```{sql, connection=db, echo=T}
select p.id, p.forked_from, p.forked_commit_id 
from projects p, users u
where u.id = p.owner_id
  and u.login = 'herflis'
  and p.name = 'rxjs-1'
```

## Get all commits for a fork (3)
```{sql, connection=db, echo=T}
select id, author, sha, created_at
from (
  select c.id, u1.login as author, c.sha, c.created_at
  from users u1, commits c, project_commits pc
  where pc.commit_id = c.id
    and pc.project_id = 450
    and u1.id = c.author_id
  union
  select c.id, u1.login as author, c.sha, c.created_at
  from users u1, commits c, project_commits pc
  where pc.commit_id = c.id
    and pc.project_id = 1
    and u1.id = c.author_id
    and c.created_at <
      (select created_at from commits where id = 3)
) as commits
order by created_at desc
```

## Basic info about forks

```{sql, connection=db, echo=T}
select u.login, p.name,  p.forked_commit_id
from projects p, users u
where p.forked_from is not null
and u.id = p.owner_id
order by p.id desc
```

## Which forks contributed code?
```{sql, connection=db, echo=T, eval=F}
```

## What are the core team members?
```{sql, connection=db, echo=T, eval=F}
```

## Which countries contributed most commits?

```{sql, connection=db, echo=T}
select u.country_code, count(*) num_commits
from users u, commits c
where u.id = c.author_id
  and country_code is not null
group by u.country_code
order by num_commits desc
```

## Using GHTorrent effectively

### Combining MongoDB and MySQL data

## Real-time data analysis

## Avoiding common pitfals

* GHTorrent (or any GitHub dataset) is not an exact GitHub replica

You can retrieve a set of data and write scripts to fill in the gaps
using the GitHub API/GraphQL.

* Don't select repositories to analyze manually

This will lead to non-reprentativeness. Define a population first,
then use _stratified sampling_ to select repositories from it.

## Avoiding common pitfals

* Repositories are not projects

A large project, such as Homebrew, is usually composed of multiple
repositories. Some repositories have equally active forks.
To study a _project_ as a community, make sure you include all its 
related activity.

* Most repositories are idle

Selecting randomly from repositories will lead to toy/dead repositories.

## Avoiding common pitfals

* Many active projects do not use GitHub exclusively

The prime example are Apache projects. Projects with with high
commit counts but not so many issues/pull requests are using other collaboration
platforms.

* Not all activity is due to users

![Retrieval scheme](packman.png){ width=90% }
Beware of bots and tool integrations, especially in issues and pull requests!

## Avoiding common pitfals 
* At least 15% of merged pull requests appear as unmerged

Use the following heuristics to uncover them

1. An entry in table `pull_request_commits` also appears in `project_commits`
2. A commit in `project_commits` references a PR number (e.g. `fixes: 12`)
3. One of the 3 last comments in the discussion match following regex
`(?:merg|appl|pull|push|integrat)(?:ing|i?ed)`
4. The above regex matches the last comment before closing the PR

## Selecting repositories to analyze

Generally, we need repositories that are active and representative. We can 
use the following selection process:

From the whole population:

1. Choose repositories that have forks ($A$)
2. from $A$, exclude repos that where inactive _recently_ ($B$)
2. from $B$, exclude repos that never received a PR ($C$)

On $C$, we can then apply further criteria (e.g. programming language, 
build system, minimum number of stars etc).

If the project list is still too long, then apply _stratified sampling_ 

## Reporeapers

A curated set of repositories worth analyzing

![Reporeapers](reporeapers.png){ width=90% }

## Privacy

```javascript
{
    "id": "4141500869",
    "type": "IssueCommentEvent",
    "actor": {},
    "repo": {},
    "payload": {
      "action": "created",
      "issue": {
        "id": 158442053,
        "number": 138,
        "title": "Issue in CopyrightedProjectName",
        "user": {},
        "labels": [],
        "state": "closed",
        "body": "Added data holding classes and a map manager. Will add a system soon"
      },
      "comment": {
        "created_at": "2016-06-14T05:51:16Z",
        "updated_at": "2016-06-14T05:51:16Z",
        "body": "continuing in #141 \r\n"
      }
    }
}
```

## Contributing to GHTorrent
